{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ROYMASON11v1/CompuBlanda/blob/main/Proyecto_Final/Reconocimiento_imagen_video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_NcOdTIERO6"
      },
      "source": [
        "##**Bienvenido**\n",
        "\n",
        "Para utilizar el modelo debera descargar y montar el siguiente archivo a los datos locales de Colab.\n",
        "\n",
        "https://drive.google.com/file/d/1KIof1vBrzcBaGQBrqx1IYxoPwkkAmndw/view?usp=sharing\n",
        "\n",
        "Luego, ejecute la seccion \"Index\".\n",
        "Tenga presente que en la sub seccion \"Configuracion\", se le pedira el cargue de dos archivos.\n",
        "\n",
        "En primer lugar, la imagen del objeto que desea buscar.\n",
        "Despues, el video sobre el cual desea realizar la busqueda.\n",
        "\n",
        "**La seccion red neuronal NO debe ser ejecutado a menos que se realicen cambios en el modelo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3H97bpi4e5u"
      },
      "source": [
        "##**Red neuronal CNN con keras y tensor**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR9d5f8d4n45"
      },
      "source": [
        "* https://pharos.sh/reconocimiento-de-imagenes-en-python-con-tensorflow-y-keras/\n",
        "\n",
        "* https://pythonistaplanet.com/cifar-10-image-classification-using-keras/\n",
        "\n",
        "* https://machinelearningmastery.com/save-load-keras-deep-learning-models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKFLjX8jtjun"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.constraints import maxnorm\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# Set random seed for purposes of reproducibility\n",
        "seed = 21\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__iqbCVctkhT",
        "outputId": "f953919e-c6cc-462b-95bc-11137ca23542"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import cifar10\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# normalize the inputs from 0-255 to between 0 and 1 by dividing by 255\n",
        "    \n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euKJ_vM-xvUX"
      },
      "outputs": [],
      "source": [
        "# one hot encode outputs\n",
        "Objects = [\"Aeroplane\", 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Ship', 'Horse', 'Truck']\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "#class_num = len(Objects)\n",
        "class_num = y_test.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSwN7ccTxv_3"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "#La primera capa de nuestro modelo es una capa convolucional. Tomará las entradas y ejecutará filtros convolucionales en ellas.\n",
        "#32 es el numero de filtros necesarios\n",
        "#un filtro es una matriz de valores numerios. En este caso el tamño del filtro es (3,3)\n",
        "\n",
        "#La imagen de entrada tiene 32*32*3, es decir:\n",
        "# 32 de altura, 32 de acho y el 3 restante se refiere a los valores RGB\n",
        "#Acada uno de los numeros de esta matriz se le dan valores de 0 a 255\n",
        "\n",
        "#El resultado de esta capa seran algunos mapas de entidades, en el cual se muestran algunas caracteristicas especificas de la imagen.\n",
        "model.add(Conv2D(32, (3, 3), input_shape=X_train.shape[1:], padding='same'))\n",
        "#Utilizamos la funcion de activacion ReLu.\n",
        "#ReLu reemplaza todos los valores de pixeles negativos en el mapa por 0.\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "#Ahora agregaremos una capa de deserción para evitar el sobreajuste, que funciona eliminando aleatoriamente algunas de las conexiones entre las capas\n",
        "model.add(Dropout(0.2))#0.2 significa que elimina el 20% de las conexiones existentes\n",
        "\n",
        "#Creamos una capa de normalizacion por lotes\n",
        "#Al normalizar las entradas que se dirigen a la siguiente capa, aseguramos que la red siempre cree activaciones con la misma distribucion que deseamos.\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "#Ahora Creamos otra capa convolucional, pero el tamaño del filtro aumenta para que la red pueda aprender representaciones más complejas\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "#Creamos una capa de agrupacion que ayudara a que el clasificador sea mas robusto para que pueda aprender patrones relevantes.\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "#Agregamos la desercion\n",
        "model.add(Dropout(0.2))\n",
        "#Agregamos la normalizacion por lotes\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "    \n",
        "model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "\n",
        "#Flatten es usado para convertir el mapa de entidades a 1 dimencsion.\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "#Ahora creamos una capa densamente conectada.\n",
        "#Es necesario espeficipar el numero de neuronas en la capa densa, recordar que en la siguientes capas este numero tendra que disminuir ...\n",
        "#... acercandose finalmente el mismo numero de neuronas que clases hay en el conjunto de datos.\n",
        "\n",
        "#La restriccion de kernal ayuda a regularizar los datos a medida que prender, por lo que se previene el sobreajuste.\n",
        "model.add(Dense(256, kernel_constraint=maxnorm(3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "  \n",
        "model.add(Dense(128, kernel_constraint=maxnorm(3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "#En esta ultima capa densamente conectada pasamos el numero de clases por el numero de neuronas. Cada neurona representa una clase, ...\n",
        "#... y la salida de esta capa sera un vector de 10 neuronas con cada neurona almacenando alguna probabilidad de que la imagen en cuestion ...\n",
        "#   ... pertenezca a la clase que representa.\n",
        "model.add(Dense(class_num))\n",
        "\n",
        "#Utilizamos la funcion de activacion softmax.\n",
        "#softmax toma como entrada un vector de k numeros y lo normaliza en una ...\n",
        "#.... distribucion de probabilidad que consiste en k probabilidades propocionales a los exponenciales de los numeros de entrada.\n",
        "model.add(Activation('softmax')) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXbbb7mTyVuk"
      },
      "outputs": [],
      "source": [
        "#Numero de epocas para las que queremos entrenar\n",
        "epochs = 30\n",
        "\n",
        "#El optimizador es lo que ajustara los pesos en su red para acercase al punto de menor perdida.\n",
        "lrate = 0.01 \n",
        "decay = lrate/epochs \n",
        "sgd = SGD(learning_rate=lrate, momentum=0.5, decay=decay, nesterov=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qPmK7HnzmFh"
      },
      "outputs": [],
      "source": [
        "#Compilamos el modelo con los parametros elegidos.\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=['accuracy'])\n",
        "#Podemos imprimir el resumen del modelo\n",
        "#print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_7LCXKX1hHn"
      },
      "outputs": [],
      "source": [
        "numpy.random.seed(seed)\n",
        "#Realizamos el entrenamiento del modelo\n",
        "#Entrenaremos con 50000 muestras y validaremos con 10000 sacadas de cifar10\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NazqfJKZ2ceh"
      },
      "outputs": [],
      "source": [
        "# Model evaluation\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fM_dcNTVAOu9"
      },
      "outputs": [],
      "source": [
        "\n",
        "from keras.models import model_from_json\n",
        "import numpy as np\n",
        "\n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"/content/model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"/content/model.h5\")\n",
        "print(\"Saved model to disk\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1Koxq3dLcc4"
      },
      "source": [
        "#**Index**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZtalWo5ivOW"
      },
      "source": [
        "##**Inicializar Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DEWhhI_i3qI",
        "outputId": "df09a166-b2bf-498b-9b97-c656083942d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "title: model.h5, id: 1dwjK2mmfOe-N3dZkEzzTLP7FLErB4lvf\n",
            "downloading to /content/model.h5\n",
            "title: model.json, id: 1dXfYh76X778nzVmJDSWyV4JN8gamSTJU\n",
            "downloading to /content/model.json\n"
          ]
        }
      ],
      "source": [
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# choose a local (colab) directory to store the data.\n",
        "local_download_path = os.path.expanduser('/content')\n",
        "try:\n",
        "  os.makedirs(local_download_path)\n",
        "except: pass\n",
        "\n",
        "# 2. Auto-iterate using the query syntax\n",
        "#    https://developers.google.com/drive/v2/web/search-parameters\n",
        "file_list = drive.ListFile(\n",
        "    {'q': \"'1hhhdROS0AvUzpnLLGbNNqo7P3u-5srWv' in parents\"}).GetList()\n",
        "\n",
        "for f in file_list:\n",
        "  # 3. Create & download by id.\n",
        "  print('title: %s, id: %s' % (f['title'], f['id']))\n",
        "  fname = os.path.join(local_download_path, f['title'])\n",
        "  print('downloading to {}'.format(fname))\n",
        "  f_ = drive.CreateFile({'id': f['id']})\n",
        "  f_.GetContentFile(fname)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xlGbIG_q1h8"
      },
      "source": [
        "##**Main**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_jvwPdQrHPl"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from keras.models import load_model\n",
        "import numpy as np \n",
        "from keras.preprocessing import image\n",
        "from keras.models import model_from_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpJphY-CBX5j",
        "outputId": "61d69bc0-7305-41c5-df07-60c8731cd406"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n"
          ]
        }
      ],
      "source": [
        "# load json and create model\n",
        "json_file = open('/content/model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"/content/model.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        "\n",
        "loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#loaded_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uULt3JMD5nc"
      },
      "outputs": [],
      "source": [
        "#Objetos posibles\n",
        "Objects = [\"Avion\", 'Automovil', 'Pajaro', 'Gato', 'Ciervo', 'Perro', 'Rana', 'Barco', 'Caballo', 'Camion']\n",
        "#Direccion\n",
        "path = '/content/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TM44Hm6CMFZ"
      },
      "outputs": [],
      "source": [
        "def ObjectImage(PathIMG):\n",
        "  test_image1 =image.load_img(path + PathIMG,target_size =(32,32))\n",
        "  test_image =image.img_to_array(test_image1) \n",
        "\n",
        "  test_image = test_image/255.0\n",
        "\n",
        "  test_image =np.expand_dims(test_image, axis =0)\n",
        "\n",
        "  result = loaded_model.predict(test_image)\n",
        "\n",
        "  ObjectPosible = -1\n",
        "\n",
        "  for i in range(10):\n",
        "    if i == 0 or result[0][i] > result[0][ObjectPosible]:\n",
        "      ObjectPosible = i\n",
        "    \n",
        "  print(\"Se detecto un \"+Objects[ObjectPosible]+'\\n')\n",
        "  return ObjectPosible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14o_kkUVs9T7"
      },
      "outputs": [],
      "source": [
        "def ExtractFrames(PathVideo):\n",
        "\n",
        "  capture =cv2.VideoCapture(path + PathVideo)\n",
        "  contFrame = 0\n",
        "  PerSecond = 0\n",
        "  \n",
        "  while (capture.isOpened()):\n",
        "    \n",
        "    PerSecond = capture.get(cv2.CAP_PROP_FPS)\n",
        "    ret, frame = capture.read()\n",
        "\n",
        "    if (ret == True):\n",
        "      cv2.imwrite(path + 'IMG_%04d.jpg' % contFrame, frame)\n",
        "      contFrame += 1\n",
        "      if (cv2.waitKey(1) == ord('s')):\n",
        "        break\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  capture.release()\n",
        "  cv2.destroyAllWindows()\n",
        "\n",
        "  return contFrame, PerSecond"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zdm1orWnL6U4"
      },
      "outputs": [],
      "source": [
        "def FrameObjectComparison(CodeObjectImage, FramePath):\n",
        "  frame_image1 =image.load_img(path + FramePath,target_size =(32,32))\n",
        "  frame_image =image.img_to_array(frame_image1) \n",
        "\n",
        "  frame_image = frame_image/255.0\n",
        "\n",
        "  frame_image =np.expand_dims(frame_image, axis =0)\n",
        "\n",
        "  result = loaded_model.predict(frame_image)\n",
        "\n",
        "  ObjectPosible = -1\n",
        "\n",
        "  for i in range(10):\n",
        "    if i == 0 or result[0][i] > result[0][ObjectPosible]:\n",
        "      ObjectPosible = i\n",
        "\n",
        "  return (ObjectPosible == CodeObjectImage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZvR01z-aMOX"
      },
      "outputs": [],
      "source": [
        "def Comparison(ContFrame, CodeObjectImage):\n",
        "  ObjectPresence = []\n",
        "  for i in range(ContFrame):\n",
        "    FramePath = ('IMG_%04d.jpg' % i)\n",
        "\n",
        "    if FrameObjectComparison(CodeObjectImage, FramePath)==True:\n",
        "      ObjectPresence.append(i)\n",
        "\n",
        "  return ObjectPresence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQQTu2mcReyY"
      },
      "outputs": [],
      "source": [
        "def PrintFrames(ObjectPresence, PerSecond):\n",
        "  print('\\n')\n",
        "  for i in ObjectPresence:\n",
        "    print('Frame at '+ str(i*(1/PerSecond)) +' second.')\n",
        "\n",
        "    image=cv2.imread(path + ('IMG_%04d.jpg' % i))\n",
        "    cv2_imshow(image)\n",
        "\n",
        "    print('\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DUV6JnmIC3z"
      },
      "outputs": [],
      "source": [
        "def Config(PathIMG, PathVideo):\n",
        "  CodeObjectImage = ObjectImage(PathIMG)\n",
        "  ContFrames, PerSecond = ExtractFrames(PathVideo)\n",
        "  ObjectPresence = Comparison(ContFrames, CodeObjectImage)\n",
        "\n",
        "  print('Se encontro un '+Objects[CodeObjectImage]+' en '+str(len(ObjectPresence))+' frames.')\n",
        "  PrintFrames(ObjectPresence, PerSecond)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC850ApDHJi9"
      },
      "source": [
        "##**Configuracion**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "THZzhkSvGplA",
        "outputId": "a60fa54a-6c16-4b28-9564-d668bf7792bf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2612a434-beb4-4f73-8b0d-7fbb03cf7077\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2612a434-beb4-4f73-8b0d-7fbb03cf7077\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving TestImage.jpg to TestImage.jpg\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a26c66d8-77ce-4411-8cd6-e1289e92a4e8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a26c66d8-77ce-4411-8cd6-e1289e92a4e8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving WhatsApp Video 2021-12-01 at 1.52.45 PM.mp4 to WhatsApp Video 2021-12-01 at 1.52.45 PM.mp4\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "Img = files.upload()\n",
        "Vid = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eZmOGKz2HTwu"
      },
      "outputs": [],
      "source": [
        "nameImg = None\n",
        "namevideo = None\n",
        "for fn in Img.keys():\n",
        "  nameImg = fn\n",
        "\n",
        "for fn in Vid.keys():\n",
        "  namevideo = fn\n",
        "\n",
        "PathIMG = nameImg\n",
        "PathVideo = namevideo\n",
        "\n",
        "Config(PathIMG, PathVideo)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "N_NcOdTIERO6",
        "p3H97bpi4e5u",
        "JZtalWo5ivOW",
        "-xlGbIG_q1h8"
      ],
      "name": "Reconocimiento_imagen_video.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}